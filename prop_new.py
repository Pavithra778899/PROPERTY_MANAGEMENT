```python
import streamlit as st
import json
import re
import requests
import snowflake.connector
import pandas as pd
from snowflake.snowpark import Session
from snowflake.core import Root
from typing import List, Dict, Any, Optional, Tuple
import plotly.express as px
import time
import yaml

# Snowflake/Cortex Configuration
HOST = "GBJYVCT-LSB50763.snowflakecomputing.com"
DATABASE = "AI"
SCHEMA = "DWH_MART"
API_ENDPOINT = "/api/v2/cortex/agent:run"
API_TIMEOUT = 50000  # in milliseconds
CORTEX_SEARCH_SERVICES = "AI.DWH_MART.propertymanagement"
CECON_SEARCH_SERVICES = "AI.DWH_MART.propertymanagement"
SEMANTIC_MODEL = '@"AI"."DWH_MART"."PROPERTY_MANAGEMENT"/property_management.yaml'

# Model options
MODELS = [
    "mistral-large",
    "snowflake-arctic",
    "llama3-70b",
    "llama3-8b",
]

# Streamlit Page Config
st.set_page_config(
    page_title="Welcome to Cortex AI Assistant",
    layout="wide",
    initial_sidebar_state="auto"
)

# Initialize session state
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False
    st.session_state.username = ""
    st.session_state.password = ""
    st.session_state.CONN = None
    st.session_state.snowpark_session = None
    st.session_state.chat_history = []
    st.session_state.messages = []
if "debug_mode" not in st.session_state:
    st.session_state.debug_mode = False
if "last_suggestions" not in st.session_state:
    st.session_state.last_suggestions = []
if "chart_x_axis" not in st.session_state:
    st.session_state.chart_x_axis = None
if "chart_y_axis" not in st.session_state:
    st.session_state.chart_y_axis = None
if "chart_type" not in st.session_state:
    st.session_state.chart_type = "Bar Chart"
if "current_query" not in st.session_state:
    st.session_state.current_query = None
if "current_results" not in st.session_state:
    st.session_state.current_results = None
if "current_sql" not in st.session_state:
    st.session_state.current_sql = None
if "current_summary" not in st.session_state:
    st.session_state.current_summary = None
if "service_metadata" not in st.session_state:
    st.session_state.service_metadata = []
if "selected_cortex_search_service" not in st.session_state:
    st.session_state.selected_cortex_search_service = CORTEX_SEARCH_SERVICES
if "model_name" not in st.session_state:
    st.session_state.model_name = "mistral-large"
if "num_retrieved_chunks" not in st.session_state:
    st.session_state.num_retrieved_chunks = 100
if "num_chat_messages" not in st.session_state:
    st.session_state.num_chat_messages = 10
if "use_chat_history" not in st.session_state:
    st.session_state.use_chat_history = True
if "clear_conversation" not in st.session_state:
    st.session_state.clear_conversation = False
if "show_selector" not in st.session_state:
    st.session_state.show_selector = False
if "show_greeting" not in st.session_state:
    st.session_state.show_greeting = True
if "data_source" not in st.session_state:
    st.session_state.data_source = "Database"
if "verified_questions" not in st.session_state:
    st.session_state.verified_questions = []

def stream_text(text: str, chunk_size: int = 4, delay: float = 0.04):
    try:
        for i in range(0, len(text), chunk_size):
            yield text[i:i + chunk_size]
            time.sleep(delay)
    except Exception as e:
        st.error(f"‚ùå Error streaming text: {str(e)}")

def start_new_conversation():
    st.session_state.chat_history = []
    st.session_state.messages = []
    st.session_state.current_query = None
    st.session_state.current_results = None
    st.session_state.current_sql = None
    st.session_state.current_summary = None
    st.session_state.chart_x_axis = None
    st.session_state.chart_y_axis = None
    st.session_state.chart_type = "Bar Chart"
    st.session_state.last_suggestions = []
    st.session_state.clear_conversation = False
    st.session_state.show_greeting = True
    st.rerun()

def generate_explanation(query: str, is_structured: bool, results: Optional[pd.DataFrame] = None, search_results: List[str] = None):
    if is_structured and results is not None:
        return f"This answer was generated by executing a SQL query on the Property Management database, retrieving {len(results)} rows of data based on your question."
    elif search_results:
        return f"This answer was derived by searching relevant documents in the Property Management module, using Snowflake Cortex Search to find matching information."
    else:
        return "This answer was generated using the selected language model, as no specific database or document results were retrieved."

def load_verified_questions():
    try:
        yaml_content = """
        verified_questions:
          - What is the total number of properties currently occupied?
          - What is the number of properties by occupancy status?
          - What is the number of properties currently leased?
          - What are the supplier payments compared to customer billing by month?
          - What is the total number of suppliers?
          - What is the average supplier payment per property?
          - What are the details of lease execution, commencement, and termination?
          - What are the customer billing and supplier payment details by location and purpose?
          - What is the budget recovery by billing purpose?
          - What are the details of customer billing?
          - What are the details of supplier payments?
        """
        data = yaml.safe_load(yaml_content)
        return data.get('verified_questions', [])
    except Exception as e:
        st.error(f"‚ùå Failed to load verified questions from YAML: {str(e)}")
        return [
            "What is the total number of properties currently occupied?",
            "What is the number of properties by occupancy status?",
            "What is the number of properties currently leased?",
            "What are the supplier payments compared to customer billing by month?",
            "What is the total number of suppliers?",
            "What is the average supplier payment per property?",
            "What are the details of lease execution, commencement, and termination?",
            "What are the customer billing and supplier payment details by location and purpose?",
            "What is the budget recovery by billing purpose?",
            "What are the details of customer billing?",
            "What are the details of supplier payments?",
        ]

def init_service_metadata():
    if not st.session_state.service_metadata:
        try:
            services = session.sql("SHOW CORTEX SEARCH SERVICES;").collect()
            service_metadata = []
            if services:
                for s in services:
                    svc_name = s["name"]
                    svc_search_col = session.sql(
                        f"DESC CORTEX SEARCH SERVICE {svc_name};"
                    ).collect()[0]["search_column"]
                    service_metadata.append(
                        {"name": svc_name, "search_column": svc_search_col}
                    )
            st.session_state.service_metadata = service_metadata
        except Exception as e:
            st.error(f"‚ùå Failed to initialize Cortex Search service metadata: {str(e)}")
            st.session_state.service_metadata = [{"name": CORTEX_SEARCH_SERVICES, "search_column": ""}]

def init_config_options():
    st.sidebar.selectbox(
        "Select cortex search service:",
        [s["name"] for s in st.session_state.service_metadata] or [CORTEX_SEARCH_SERVICES],
        key="selected_cortex_search_service"
    )
    st.sidebar.button("Clear conversation", on_click=start_new_conversation)
    st.sidebar.toggle("Debug", key="debug_mode", value=st.session_state.debug_mode)
    st.sidebar.toggle("Use chat history", key="use_chat_history", value=True)
    with st.sidebar.expander("Advanced options"):
        st.selectbox("Select model:", MODELS, key="model_name")
        st.number_input(
            "Select number of context chunks",
            value=100,
            key="num_retrieved_chunks",
            min_value=1,
            max_value=400
        )
        st.number_input(
            "Select number of messages to use in chat history",
            value=10,
            key="num_chat_messages",
            min_value=1,
            max_value=100
        )
    if st.session_state.debug_mode:
        with st.sidebar.expander("Session State"):
            st.write(st.session_state)

def query_cortex_search_service(query: str):
    try:
        db, schema = session.get_current_database(), session.get_current_schema()
        root = Root(session)
        cortex_search_service = (
            root.databases[db]
            .schemas[schema]
            .cortex_search_services[st.session_state.selected_cortex_search_service]
        )
        context_documents = cortex_search_service.search(
            query, columns=[], limit=st.session_state.num_retrieved_chunks
        )
        results = context_documents.results
        service_metadata = st.session_state.service_metadata
        search_col = [s["search_column"] for s in service_metadata
                      if s["name"] == st.session_state.selected_cortex_search_service][0]
        context_str = ""
        for i, r in enumerate(results):
            context_str += f"Context document {i+1}: {r[search_col]}\n\n"
        if st.session_state.debug_mode:
            st.sidebar.text_area("Context documents", context_str, height=400)
        return context_str
    except Exception as e:
        st.error(f"‚ùå Error querying Cortex Search service: {str(e)}")
        return ""

def get_chat_history():
    start_index = max(
        0,
        len(st.session_state.chat_history) - st.session_state.num_chat_messages
    )
    return st.session_state.chat_history[start_index:]

def make_chat_history_summary(chat_history: List[Dict[str, Any]], question: str):
    chat_history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
    prompt = f"""
        [INST]
        Based on the chat history below and the question, generate a query that extends the question
        with the chat history provided. The query should be in natural language.
        Answer with only the query. Do not add any explanation.

        <chat_history>
        {chat_history_str}
        </chat_history>
        <question>
        {question}
        </question>
        [/INST]
    """
    summary = complete(st.session_state.model_name, prompt)
    if st.session_state.debug_mode:
        st.sidebar.text_area("Chat history summary", summary.replace("$", "\$"), height=150)
    return summary

def create_prompt(user_question):
    chat_history_str = ""
    if st.session_state.use_chat_history:
        chat_history = get_chat_history()
        if chat_history:
            question_summary = make_chat_history_summary(chat_history, user_question)
            prompt_context = query_cortex_search_service(question_summary)
            chat_history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
        else:
            prompt_context = query_cortex_search_service(user_question)
    else:
        prompt_context = query_cortex_search_service(user_question)
    
    if not prompt_context.strip():
        return complete(st.session_state.model_name, user_question)
    
    prompt = f"""
        [INST]
        You are a helpful AI chat assistant with RAG capabilities. When a user asks you a question,
        you will also be given context provided between <context> and </context> tags. Use that context
        with the user's chat history provided in the between <chat_history> and </chat_history> tags
        to provide a summary that addresses the user's question. Ensure the answer is coherent, concise,
        and directly relevant to the user's question.

        If the user asks a generic question which cannot be answered with the given context or chat_history,
        just respond directly and concisely to the user's question using the LLM.

        <chat_history>
        {chat_history_str}
        </chat_history>
        <context>
        {prompt_context}
        </context>
        <question>
        {user_question}
        </question>
        [/INST]
        Answer:
    """
    return complete(st.session_state.model_name, prompt)

def is_off_topic_query(query: str):
    module_keywords = [
        r'\b(property|properties|lease|leases|tenant|tenants|rent|rental|occupancy|supplier|suppliers|payment|billing|customer|location|purpose|budget)\b',
    ]
    return not any(re.search(pattern, query.lower()) for pattern in module_keywords)

if not st.session_state.authenticated:
    st.title("Welcome to Snowflake Cortex AI")
    st.markdown("Please login to interact with your data")
    st.session_state.username = st.text_input("Enter Snowflake Username:", value=st.session_state.username)
    st.session_state.password = st.text_input("Enter Password:", type="password")
    if st.button("Login"):
        try:
            conn = snowflake.connector.connect(
                user=st.session_state.username,
                password=st.session_state.password,
                account="GBJYVCT-LSB50763",
                host=HOST,
                port=443,
                warehouse="COMPUTE_WH",
                role="ACCOUNTADMIN",
                database=DATABASE,
                schema=SCHEMA,
            )
            st.session_state.CONN = conn
            snowpark_session = Session.builder.configs({
                "connection": conn
            }).create()
            st.session_state.snowpark_session = snowpark_session
            with conn.cursor() as cur:
                cur.execute(f"USE DATABASE {DATABASE}")
                cur.execute(f"USE SCHEMA {SCHEMA}")
                cur.execute("ALTER SESSION SET TIMEZONE = 'Asia/Kolkata'")
                cur.execute("ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = TRUE")
            st.session_state.authenticated = True
            st.success("Authentication successful! Redirecting...")
            st.rerun()
        except Exception as e:
            st.error(f"Authentication failed: {e}")
else:
    session = st.session_state.snowpark_session
    root = Root(session)

    if not st.session_state.verified_questions:
        st.session_state.verified_questions = load_verified_questions()

    def run_snowflake_query(query):
        try:
            if not query:
                return None
            df = session.sql(query)
            data = df.collect()
            if not data:
                if st.session_state.debug_mode:
                    st.sidebar.warning("Query returned no data.")
                return None
            columns = df.schema.names
            result_df = pd.DataFrame(data, columns=columns)
            if st.session_state.debug_mode:
                st.sidebar.text_area("Query Results", result_df.to_string(), height=200)
            return result_df
        except Exception as e:
            st.error(f"‚ùå SQL Execution Error: {str(e)}")
            if st.session_state.debug_mode:
                st.sidebar.error(f"SQL Error Details: {str(e)}")
            return None

    def is_structured_query(query: str):
        structured_patterns = [
            r'\b(count|number|where|group by|order by|sum|avg|max|min|total|how many|which|show|list|names?|are there any|rejected deliveries?|least|highest|duration|approval)\b',
            r'\b(vendor|supplier|requisition|purchase order|po|organization|department|buyer|delivery|received|billed|rejected|late|on time|late deliveries?|Suppliers|payment|billing|percentage|list)\b'
        ]
        return any(re.search(pattern, query.lower()) for pattern in structured_patterns)

    def is_complete_query(query: str):
        complete_patterns = [r'\b(generate|write|create|describe|explain)\b']
        return any(re.search(pattern, query.lower()) for pattern in complete_patterns)

    def is_summarize_query(query: str):
        summarize_patterns = [r'\b(summarize|summary|condense)\b']
        return any(re.search(pattern, query.lower()) for pattern in summarize_patterns)

    def is_question_suggestion_query(query: str):
        suggestion_patterns = [
            r'\b(what|which|how)\b.*\b(questions|type of questions|queries)\b.*\b(ask|can i ask|pose)\b',
            r'\b(give me|show me|list)\b.*\b(questions|examples|sample questions)\b'
        ]
        return any(re.search(pattern, query.lower()) for pattern in suggestion_patterns)

    def is_greeting_query(query: str):
        greeting_patterns = [
            r'^\b(hello|hi|hey|greet)\b$',
            r'^\b(hello|hi|hey|greet)\b\s.*$'
        ]
        return any(re.search(pattern, query.lower()) for pattern in greeting_patterns)

    def complete(model, prompt):
        try:
            prompt = prompt.replace("'", "\\'")
            query = f"SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', '{prompt}') AS response"
            result = session.sql(query).collect()
            return result[0]["RESPONSE"]
        except Exception as e:
            st.error(f"‚ùå COMPLETE Function Error: {str(e)}")
            return None

    def summarize(text):
        try:
            text = text.replace("'", "\\'")
            query = f"SELECT SNOWFLAKE.CORTEX.SUMMARIZE('{text}') AS summary"
            result = session.sql(query).collect()
            return result[0]["SUMMARY"]
        except Exception as e:
            st.error(f"‚ùå SUMMARIZE Function Error: {str(e)}")
            return None

    def parse_sse_response(response_text: str) -> List[Dict]:
        events = []
        lines = response_text.strip().split("\n")
        current_event = {}
        for line in lines:
            if line.startswith("event:"):
                current_event["event"] = line.split(":", 1)[1].strip()
            elif line.startswith("data:"):
                data_str = line.split(":", 1)[1].strip()
                if data_str != "[DONE]":
                    try:
                        data_json = json.loads(data_str)
                        current_event["data"] = data_json
                        events.append(current_event)
                        current_event = {}
                    except json.JSONDecodeError as e:
                        st.error(f"‚ùå Failed to parse SSE data: {str(e)} - Data: {data_str}")
        return events

    def process_sse_response(response, is_structured):
        sql = ""
        search_results = []
        if not response:
            return sql, search_results
        try:
            for event in response:
                if event.get("event") == "message.delta" and "data" in event:
                    delta = event["data"].get("delta", {})
                    content = delta.get("content", [])
                    for item in content:
                        if item.get("type") == "tool_results":
                            tool_results = item.get("tool_results", {})
                            if "content" in tool_results:
                                for result in tool_results["content"]:
                                    if result.get("type") == "json":
                                        result_data = result.get("json", {})
                                        if is_structured and "sql" in result_data:
                                            sql = result_data.get("sql", "")
                                        elif not is_structured and "searchResults" in result_data:
                                            search_results = [sr["text"] for sr in result_data.get("searchResults", [])]
        except Exception as e:
            st.error(f"‚ùå Error Processing Response: {str(e)}")
        return sql.strip(), search_results

    def snowflake_api_call(query: str, is_structured: bool = False):
        payload = {
            "model": st.session_state.model_name,
            "messages": [{"role": "user", "content": [{"type": "text", "text": query}]}],
            "tools": []
        }
        if is_structured:
            payload["tools"].append({"tool_spec": {"type": "cortex_analyst_text_to_sql", "name": "analyst1"}})
            payload["tool_resources"] = {"analyst1": {"semantic_model_file": SEMANTIC_MODEL}}
        else:
            payload["tools"].append({"tool_spec": {"type": "cortex_search", "name": "search1"}})
            payload["tool_resources"] = {"search1": {"name": st.session_state.selected_cortex_search_service, "max_results": st.session_state.num_retrieved_chunks}}
        try:
            resp = requests.post(
                url=f"https://{HOST}{API_ENDPOINT}",
                json=payload,
                headers={
                    "Authorization": f'Snowflake Token="{st.session_state.CONN.rest.token}"',
                    "Content-Type": "application/json",
                },
                timeout=API_TIMEOUT // 1000
            )
            if st.session_state.debug_mode:
                st.write(f"API Response Status: {resp.status_code}")
                st.write(f"API Raw Response: {resp.text}")
            if resp.status_code < 400:
                if not resp.text.strip():
                    st.error("‚ùå API returned an empty response.")
                    return None
                return parse_sse_response(resp.text)
            else:
                raise Exception(f"Failed request with status {resp.status_code}: {resp.text}")
        except Exception as e:
            st.error(f"‚ùå API Request Error: {str(e)}")
            return None

    def summarize_unstructured_answer(answer):
        sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|")\s', answer)
        return "\n".join(f"‚Ä¢ {sent.strip()}" for sent in sentences[:6] if sent.strip())

    def suggest_sample_questions(query: str) -> List[str]:
        try:
            query_lower = query.lower()
            verified_questions = st.session_state.verified_questions
            scored_questions = []
            for q in verified_questions:
                q_lower = q.lower()
                score = sum(1 for word in query_lower.split() if word in q_lower)
                scored_questions.append((score, q))
            scored_questions.sort(reverse=True)
            selected_questions = [q for _, q in scored_questions[:3]]
            while len(selected_questions) < 3 and verified_questions:
                remaining = [q for q in verified_questions if q not in selected_questions]
                if remaining:
                    selected_questions.append(remaining[0])
            return selected_questions[:3]
        except Exception as e:
            st.error(f"‚ùå Failed to generate sample questions: {str(e)}")
            return st.session_state.verified_questions[:3]

    def display_chart(df: pd.DataFrame, prefix: str = "chart", query: str = ""):
        try:
            if df is None or df.empty or len(df.columns) < 2:
                st.warning("No valid data available for visualization.")
                if st.session_state.debug_mode:
                    st.sidebar.warning(f"Chart Data Issue: df={df}, columns={df.columns if df is not None else 'None'}")
                return
            query_lower = query.lower()
            if re.search(r'\b(county|jurisdiction)\b', query_lower):
                default_chart = "Pie Chart"
            elif re.search(r'\b(month|year|date)\b', query_lower):
                default_chart = "Line Chart"
            else:
                default_chart = "Bar Chart"
            all_cols = list(df.columns)
            col1, col2, col3 = st.columns(3)
            x_col = col1.selectbox("X axis", all_cols, index=0, key=f"{prefix}_x")
            remaining_cols = [c for c in all_cols if c != x_col]
            y_col = col2.selectbox("Y axis", remaining_cols, index=0 if remaining_cols else None, key=f"{prefix}_y")
            chart_options = ["Line Chart", "Bar Chart", "Pie Chart", "Scatter Chart", "Histogram Chart"]
            chart_type = col3.selectbox("Chart Type", chart_options, index=chart_options.index(default_chart), key=f"{prefix}_type")
            if st.session_state.debug_mode:
                st.sidebar.text_area("Chart Config", f"X: {x_col}, Y: {y_col}, Type: {chart_type}", height=100)
            if chart_type == "Line Chart":
                fig = px.line(df, x=x_col, y=y_col, title=f"{chart_type}: {x_col} vs {y_col}")
                st.plotly_chart(fig, key=f"{prefix}_line")
            elif chart_type == "Bar Chart":
                fig = px.bar(df, x=x_col, y=y_col, title=f"{chart_type}: {x_col} vs {y_col}")
                st.plotly_chart(fig, key=f"{prefix}_bar")
            elif chart_type == "Pie Chart":
                fig = px.pie(df, names=x_col, values=y_col, title=f"{chart_type}: {x_col}")
                st.plotly_chart(response_content, key=f"{prefix}_pie")
            elif chart_type == "Scatter Chart":
                fig = px.scatter(df, x=x_col, y=y_col, title=f"{chart_type}: {x_col} vs {y_col}")
                st.plotly_chart(fig, key=f"{prefix}_scatter")
            elif chart_type == "Histogram Chart":
                fig = px.histogram(df, x=x_col, title=f"{chart_type}: {x_col}")
                st.plotly_chart(fig, key=f"{prefix}_hist")
        except Exception as e:
            st.error(f"Error generating chart: {str(e)}")
            if st.session_state.debug_mode:
                st.sidebar.error(f"Chart Error Details: {str(e)}")

    with st.sidebar:
        st.markdown("""
        <style>
        [data-testid="stSidebar"] [data-testid="stButton"] > button {
            background-color: #29B5E8 !important;
            color: white !important;
            width: 100% !important;
            font-weight: bold !important;
            border-radius: 0px !important;
            margin: 0 !important;
            padding: none !important;
            border: 0.5rem 1rem !important;
        }
        </style>
        """, unsafe_content="html=True")

    logo_container = st.container()
    button_container = st.container()
    about_container = st.container()
    with logo_container:
        with logo_container:
            with st.container():
                content = logo_url="https://www.snowflake.com/wp-content/themes/snowflake/assets/images/logo-blue.png"
                st.image(content, width=250)

    with button_container:
        init_config_options()
        st.radio("Select Data Source:", ["Database", "Document"], key="data_source")

    with about_container:
        st.markdown("### About
")
        st.markdown(
            """
            This application uses **Snowflake Cortex** to interpret your natural language questions and generate data insights based on The **Property Management module**. Simply ask a
            question about your property management data to see relevant answers and visualizations.
            """
)
        st.markdown("### Help & Documentation")
        st.write(
            """
            - [User Guide](https://docs.snowflake.com/en/guides-overview-ai-features)
            - [Snowflake Cortex Docs](https://docs.snowflake.com/)
            - [Contact Support](https://support.snowflake.com/contact-support/)
            """
)

    st.title("Property Management Assistant by DiLytics")
    semantic_model_name = SEMANTIC_MODEL.split("/")[-1]
    st.markdown(f"semantic model: `{semantic_model_filename}`")
    init_service_metadata()

    if st.session_state.show_greeting and not st.session_state.chat_history:
        st.markdown("""
        **Welcome to the Property Management Assistant!**  
        Ask your questions about property management, such as occupancy rates, lease details, or tenant payments, and get insights from your data.  
        Try one of the suggested questions from the sidebar or type your own!
        """)

    st.sidebar.subheader("Suggested Questions")
    sample_questions = st.session_state.verified_questions.copy()

    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_content_type="html=True")
            if message["role"] == ["assistant"] and "results" in message and message["results"]:
                with st.expander("View SQL Query", expand=False):
                    st.code(message["sql"], language="sql")
                st.markdown(f"**Query Results ({len(message["results"])} rows):**")
                st.dataframe(message["results"])
                if not message["results"].empty and len(message["results"].columns) >= 2:
                    st.markdown("**üìà Visualization:")
                    st.markdown("### Query Results")
                    st.markdown(response_content, unsafe_content="results")
                    display_chart(message["results"], prefix=f"chart_{hash(message["content"])}", query=message.get("query", ""))

    query = st.chat_input("Ask your question...")
    if query and query.lower().startswith("no of"):
        query = query.replace("no of", "number of", 1)
    for sample in sample_questions:
        if st.sidebar.button(sample, key=f"sample_{hash(sample)}"):
            query = sample
            st.session_state.show_greeting = False

    if query:
        st.session_state.show_greeting = False
        st.session_state.chart_x_axis = None
        st.session_state.chart_y_axis = None
        original_query = query
        if query.strip().isdigit() and st.session_state.current_suggestions:
            try:
                index = int(query.strip()) - 1
                if 0 <= index < len(st.session_state.last_suggestions):
                    query = st.session_state.last_suggestions[index]
                else:
                    query = original_query
            except ValueError:
                query = original_query

        st.session_state.chat_history.append({"role": "user", "content": original_query})
        st.session_state.messages.append({"role": "user", "content": original_query})

        with st.chat_message("user"):
            st.markdown(original_query)

        with st.chat_message("assistant"):
            with st.spinner("Generating Response..."):
                response_content = st.empty()
                is_structured = is_structured_query(query) and st.session_state.data_source == "Database"
                is_complete = is_complete_query(query)
                is_summerize = is_summarize_query(query)
                is_suggestion = is_question_suggestion_query(query)
                is_greeting = is_greeting_query(query)
                is_off_topic = is_off_topic_query(query)
                assistant_response = {"role": "assistant", "content": "", "query": query}
                response_content = ""
                failed_response = False

                if is_greeting and original_query.lower().strip() == "hi":
                    response_content = """
                    Hi! I'm here to assist you!  
                    The **Property Management module** enables you to analyze data related to properties, leases, tenants, payments, and occupancy metrics. Ask about occupancy rates, 
                    lease terms, tenant payments, or supplier details to get started!
                    """
                    with response_content(response_content):
                        for chunk in stream_text(response_content):
                            response_content.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_content=True)
                        st.markdown(response_content, unsafe_content="html=True")
                    assistant_response["content"] = response_content
                    st.session_state.messages.append({"role": "assistant", "content": response_content})
                    st.session_state.current_suggestions = sample_questions[:5]

                elif is_greeting or is_suggestion:
                    greeting = query.lower().split()[0].capitalize()
                    if greeting not in ["Hi", "Hello", "Hey"]:
                        greeting = "Hi"
                    response_content = f"{greeting}! I'm here to help with your property management questions. Here are some questions you can ask me:\n\n"
                    for i, q in enumerate(sample_questions[:5], 1):
                        response_content += f"{i}. {q}\n"
                    response_content += "\nFeel free to ask any of these or try your own question related to property management!"
                    with response_content(response_content):
                        for chunk in stream_text(response_content):
                            response_content.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_content=True)
                        st.markdown(response_content, unsafe_content="html=True")
                    assistant_response["content"] = response_content
                    st.session_state.current_suggestions = sample_questions[:5]
                    st.session_state.messages.append({"role": "assistant", "content": response_content})

                elif is_off_topic and not (is_complete or is_summerize):
                    response_content = "There is no information related to that topic. Here are some relevant questions you can ask about the Property Management module:\n\n"
                    selected_questions = st.session_state.verified_questions[:5]
                    for i, q in enumerate(selected_questions, 1):
                        response_content += f"{i}. {q}\n"
                    response_content += "\nTry one of these questions or rephrase your query to focus on property management topics!"
                    with response_content(response_content):
                        for chunk in stream_text(response_content):
                            response_content.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_content=True)
                        st.markdown(response_content, unsafe_content="html=True")
                    assistant_response["content"] = response_content
                    st.session_state.current_suggestions = selected_questions
                    st.session_state.messages.append({"role": "assistant", "content": response_content})

                elif is_complete:
                    response = create_prompt(query)
                    if response:
                        explanation = response_content_generator(query, is_structured=False)
                        suggestions = sample_questions(query)
                        response_content = f"""
                        **Explanation:**  
                        {explanation}

                        **Answer:**  
                        {response}

                        **More Suggested Questions:**  
                        1. {suggestions[0]}  
                        2. {suggestions[1]}  
                        3. {suggestions[2]}
                        """
                        with response_content(response_content):
                            for chunk in stream_text(response_content):
                                response_content.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_content=True)
                            st.markdown(response_content, unsafe_content="html=True")
                        assistant_response["content"] = response_content
                        st.session_state.messages.append({"role": "assistant", "content": response_content})
                        st.session_state.current_suggestions = suggestions
                    else:
                        response_content = ""
                        failed_response = True
                        assistant_response["content"] = response_content

                elif is_summerize:
                    summary = summarize(query)
                    if summary:
                        explanation = summary
                        suggestions = sample_questions(query)
                        response_content = f"""
                        **Explanation:**  
                        {explanation}

                        **Summary:**  
                        {summary}

                        **More Suggested Questions:**  
                        1. {suggestions[0]}  
                        2. {suggestions[1]}  
                        3. {suggestions[2]}
                        """
                        with response_content(response_content):
                            for chunk in stream_text(response_content):
                                response_content.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_content=True)
                            st.markdown(response_content, unsafe_content="html=True")
                        assistant_response["content"] = response_content
                        st.session_state.messages.append({"role": "assistant", "content": response_content})
                        st.session_state.current_suggestions = suggestions
                    else:
                        response_content = ""
                        failed_response = True
                        assistant_response["content"] = response_content

                elif st.session_state.data_source == "Database" and is_structured:
                    response = snowflake_api_call(query, is_structured=True)
                    sql, _ = process_sse_response(response, is_structured=True)
                    if sql:
                        if st.session_state.debug_mode:
                            st.sidebar.text_area("Generated SQL", sql, height=150)
                        results = run_snowflake_query(sql)
                        if results is not None and not results.empty:
                            results_text = results.to_string(index=False)
                            prompt = f"Provide a concise natural language answer to the query '{query}' using the given data, avoiding phrases like 'Based on the query results':\n\n{results_text}"
                            summary = complete(st.session_state.model_name, prompt)
                            if not summary:
                                summary = "‚ö†Ô∏è Unable to generate a natural language summary."
                            explanation = response_content_generator(query, is_structured=True, results=results)
                            suggestions = sample_questions(query)
                            response_content = f"""
                            **Explanation:**  
                            {explanation}

                            **Answer:**  
                            {summary}

                            **Query Results ({len(results)} rows):**
                            """
                            with response_content(response_content):
                                for chunk in stream_text(response_content):
                                    response_content.markdown(response_content[:response_content.find(chunk) + len(chunk)])
                                st.markdown(response_content)
                                with st.expander("View SQL Query", expanded=False):
                                    st.code(sql, language="sql")
                                st.dataframe(results)
                                if len(results.columns) >= 2:
                                    st.markdown("**üìà Visualization:")
                                    display_chart(results, prefix=f"chart_{hash(query)}", query=query)
                                st.markdown("\n**More Suggested Questions:**")
                                for i, suggestion in enumerate(suggestions, 1):
                                    st.markdown(f"{i}. {suggestion}")
                            assistant_response.update({
                                "content": response_content,
                                "sql": query,
                                "results": results,
                                "summary": summary
                            })
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": response_content,
                                "sql": query,
                                "results": results,
                                "summary": summary
                            })
                        else:
                            response_content = "No data returned for the query."
                            failed_response = True
                            assistant_response["content"] = response_content
                    else:
                        response_content = "No valid SQL query generated."
                        failed_response = True
                        assistant_response["content"] = response_content

                elif st.session_state.data_source == "Document":
                    response = snowflake_api_call(query)
                    _, search_results = process_sse_response(response, is_structured=False)
                    if search_results:
                        raw_result = search_results[0]
                        summary = create_prompt(query)
                        if summary:
                            explanation = generate_explanation(query, is_structured=False, search_results=search_results)
                            suggestions = suggest_sample_questions(query)
                            response_content = f"""
                            **Explanation:**  
                            {explanation}

                            **Answer:**  
                            {summary}

                            **More Suggested Questions:**  
                            1. {suggestions[0]}  
                            2. {suggestions[1]}  
                            3. {suggestions[2]}
                            """
                            with response_placeholder:
                                for chunk in stream_text(response_content):
                                    response_placeholder.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_html=True)
                                st.markdown(response_content, unsafe_html=True)
                            assistant_response["content"] = response_content
                            st.session_state.messages.append({"role": "assistant", "content": response_content})
                            st.session_state.last_suggestions = suggestions
                        else:
                            explanation = generate_explanation(query, is_structured=False, search_results=search_results)
                            suggestions = suggest_sample_questions(query)
                            response_content = f"""
                            **Explanation:**  
                            {explanation}

                            **Answer:**  
                            {summarize_unstructured_answer(raw_result)}

                            **More Suggested Questions:**  
                            1. {suggestions[0]}  
                            2. {suggestions[1]}  
                            3. {suggestions[2]}
                            """
                            with response_placeholder:
                                for chunk in stream_text(response_content):
                                    response_placeholder.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_html=True)
                                st.markdown(response_content, unsafe_html=True)
                            assistant_response["content"] = response_content
                            st.session_state.messages.append({"role": "assistant", "content": response_content})
                            st.session_state.last_suggestions = suggestions
                    else:
                        response_content = ""
                        failed_response = True
                        assistant_response["content"] = response_content

                else:
                    response_content = "Please select a valid data source to proceed with your query."
                    with response_placeholder:
                        for chunk in stream_text(response_content):
                            response_placeholder.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_html=True)
                        st.markdown(response_content, unsafe_html=True)
                    assistant_response["content"] = response_content
                    st.session_state.messages.append({"role": "assistant", "content": response_content})

                if failed_response:
                    response_content = "There is no information related to that topic. Here are some relevant questions you can ask about the Property Management module:\n\n"
                    selected_questions = st.session_state.verified_questions[:5]
                    for i, q in enumerate(selected_questions, 1):
                        response_content += f"{i}. {q}\n"
                    response_content += "\nTry one of these questions or rephrase your query to focus on property management topics!"
                    with response_placeholder:
                        for chunk in stream_text(response_content):
                            response_placeholder.markdown(response_content[:response_content.find(chunk) + len(chunk)], unsafe_html=True)
                        st.markdown(response_content, unsafe_html=True)
                    assistant_response["content"] = response_content
                    st.session_state.last_suggestions = selected_questions
                    st.session_state.messages.append({"role": "assistant", "content": response_content})

                st.session_state.chat_history.append(assistant_response)
                st.session_state.current_query = query
                st.session_state.current_results = assistant_response.get("results")
                st.session_state.current_sql = assistant_response.get("sql")
                st.session_state.current_summary = assistant_response.get("summary")
```
